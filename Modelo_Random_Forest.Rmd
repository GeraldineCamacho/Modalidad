---
title: "Random Forest"
author: "Yilber Alejandro Erazo Bolaños"
date: "11/2/2021"
output: 
    bookdown: html_document2:
      fig_caption: true
      toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = F, warning = F, fig.align = "center")
```

librería
```{r}
#install.packages("ranger")
#install.packages("tidymodels")
library(tidymodels)
library(knitr)
library(readxl)
```


# Modelo Random Forest 

## Random Forest por regresión

### Preprocesamiento de los datos

Indicadores supersalud
```{r}
supersalud <- read_excel("supersalud_indicadores.xlsx")
#supersalud <- read.csv("supersalud_indicadores.csv")
```


Lectura de los datos
```{r}
indicadores_gen <- read.csv("indicadores_BD.csv")
```

Eliminar columnas que no son necesarias
```{r}
indicadores_gen$X <- NULL
indicadores_gen$cod_entidad <- NULL #porque hay muy pccos datos de cada municipio
indicadores_gen$nit_entidad <- NULL
indicadores_gen$cod_departamento <- NULL #porque ya se tiene el ombre del departamento
indicadores_gen$municipio_entidad <- NULL #porque hay muy pocos datos de cada municipio
indicadores_gen$depto_mun <- NULL #porque hay muy pocos datos de cada municipio
indicadores_gen$perc_contr_direct_val <- NULL #porque no tenemos información sobre la contratación directa

#para volver a correr el modelo porque con esta columna genera error 
indicadores_gen$nombre_entidad <- NULL

#pasar a factor las nominales a ver si funciona el modelo
#indicadores_gen$nombre_entidad <- as.factor(indicadores_gen$nombre_entidad)
indicadores_gen$orden_entidad <- as.factor(indicadores_gen$orden_entidad)
indicadores_gen$departamento_entidad <- as.factor(indicadores_gen$departamento_entidad)

# eliminar la columna de los deptos porque no es importante 
#indicadores_gen$departamento_entidad <- NULL
# cuando se elimina los departamentos el r cuadrado baja


# ver si el modelo hace un mejor trabajo solo con los que tienen contr directa >0
#indicadores_gen <- indicadores_gen %>% filter(perc_contr_directa_num>0)
```

División de los datos en train y test
```{r}
set.seed(123)
indicadores_gen_split <- initial_split(indicadores_gen, 
                                       strata = perc_contr_directa_num,
                                       prop = 4/5)
indicadores_gen_train <- training(indicadores_gen_split)
indicadores_gen_test <- testing(indicadores_gen_split)
```

Procesamiento y preparación de variables
```{r}
indicadores_gen_rec <- recipe(perc_contr_directa_num ~ ., data = indicadores_gen) %>% 
  #update_role(nombre_entidad, new_role = "ID") %>% 
  step_normalize(all_numeric(), -perc_contr_directa_num) %>% 
  step_dummy(all_nominal(), one_hot = TRUE)

#indicadores_gen_rec
```

El objeto 'indicadores_gen_rec' es una rutina de preprocesamiento que se puede aplicar a los datos de train y test.
```{r}
indicadores_gen_train_prep <- indicadores_gen_rec %>% 
  prep(indicadores_gen_train) %>% 
  juice()

#head(indicadores_gen_train_prep)
```

### Configurar el modelo

La librería a usar es 'Parsnip'que ofrece una interfaz unificada para la gran variedad de modelos de machine learning que existen en R. Esto significa que solo tiene que utilizar una forma estandar para configurar un modelo, y puede usar esta configuración diferentes tipos de modelos: Módelo Lineal, Ridge, Lasso, Random Forest, Regresión Logística, Arboles, Support Vector Machine, entre otros.
```{r}
indicadores_gen_rf <- rand_forest() %>% 
  set_args(mtry = tune(),
           trees = tune()) %>% 
  set_engine("ranger", importance = "impurity") %>% 
  set_mode("regression")

indicadores_gen_rf %>% translate()
```
### Definir el workflow

El flujo de trabajo permite integrar todas las tareas que se estan desarrollando para entrenar el modelo de machine learning.
```{r}
rf_workflow_reg <- workflow() %>% 
  add_recipe(indicadores_gen_rec) %>% 
  add_model(indicadores_gen_rf)

#rf_workflow_reg
```

### Calibración de parámetros

Para calibrar los parametros se utiliza el metodo de validación cruzada. Este procedimiento tiene como proposito encontrar los parametros optimos del modelo o algoritmo.
```{r}
# creación de los folds
indicadores_gen_cv <- vfold_cv(indicadores_gen_train, v=5)
indicadores_gen_cv
```

Ahora se configuran las metricas que se utilizarán para calibrar los parametros del modelo en la validación cruzada. Primero se define la grilla de valores para los parametros del modelo.
```{r}
rf_gird_reg <- expand.grid(mtry = c(3, 4, 5, 7),
                           trees = c(100, 300, 500))

```

Ahora se corre un modelo de random forest para cada una de estas condiciones.
```{r}
rf_tune_results_reg <- rf_workflow_reg %>% 
  tune_grid(resamples = indicadores_gen_cv,
            grid = rf_gird_reg,
            metrics = metric_set(rmse, mae))

#rf_tune_results_reg
```

A continuación se presenta la tabla que contiene el valor de las metricas para valor de los parametros y de acuerdo a cada metrica.
```{r}
# print results
rf_metrics_reg <- rf_tune_results_reg %>% 
  collect_metrics()

rf_metrics_reg
```
A continuación se presentan un conjunto de gráfica que permiten analizar el comportamiento de las métricas para cada uno de los paramétros del modelo.
```{r}
rf_metrics_reg %>% 
  filter(.metric == "rmse") %>% 
  ggplot(aes(mtry, mean, col = factor(trees))) +
  geom_line(alpha = 0.5, size = 1) +
  ggtitle("RMSE en función del mtry y el número de árboles")
```

grafico2
```{r}
rf_metrics_reg %>% 
  filter(.metric == "mae") %>% 
  ggplot(aes(mtry, mean, col = factor(trees))) +
  geom_line(alpha = 0.5, size = 1) +
  ggtitle("RMSE en función del mtry y el número de árboles")
```

A continuación se presenta el codigo para seleccionar los mejores parametros del modelo en función de la métrica rmse:
```{r}
param_final_reg <- rf_tune_results_reg %>% 
  select_best(metric = "rmse")

param_final_reg
```

### Validación del modelo final

En primer lugar se agregan los parametros calibrado al flujo de trabajo.
```{r}
rf_workflow_reg <- rf_workflow_reg %>% 
  finalize_workflow(param_final_reg)

rf_workflow_reg
```

Ahora se valida con los conjuntos de entrenamiento y prueba. En este caso se toma todo el conjunto de entrenamiento y se construye un modelo con los parametros calibrados del random forest.
```{r}
rf_fit_reg <- rf_workflow_reg %>% 
  last_fit(indicadores_gen_split)

rf_fit_reg
```

Ahora se evalua el rmse con el conjunto de entrenamiento en el modelo final.
```{r}
test_perfomance_reg <- rf_fit_reg %>% collect_metrics()

test_perfomance_reg
```

Comparar los valores reales con las predicciones
```{r}
test_predictions_reg <- rf_fit_reg %>% 
  collect_predictions()

test_predictions_reg
```

Grafico con las predicciones
```{r}
ggplot(test_predictions_reg, aes(perc_contr_directa_num, .pred)) +
  geom_point() + 
  geom_abline(intercept = 0, slope = 1) +
  ggtitle("Valor real (contratación directa) Vs predicción (.pred) TRAIN")

cor.test(test_predictions_reg$perc_contr_directa_num, test_predictions_reg$.pred)
```
### Modelo final
```{r}
final_model <- fit(rf_workflow_reg, indicadores_gen)
final_model
```

### variables de mayor importancia
```{r}
ranger_obj <- pull_workflow_fit(final_model)$fit

rf_importacia_reg <- tibble(variable = names(ranger_obj$variable.importance),
                            importancia = ranger_obj$variable.importance)


importantes <- data.frame(rf_importacia_reg %>% 
                             mutate(variable = forcats::fct_reorder(variable, importancia)) %>% 
                            arrange(desc(importancia)))
importantes
```

Gráfico de las variables más importantes
```{r}
rf_importacia_reg %>% 
  mutate(variable = forcats::fct_reorder(variable, importancia)) %>% 
   top_n(20) %>% 
  ggplot(aes(variable, importancia)) + 
  geom_bar(stat = "identity") + 
  ggtitle("Importancia de los predictores") +
  coord_flip()
 
```
NOTA: Cuando se trata de los que SI tuvieron contratación directa el indicador de riesgo de corrupción sigue siendo el segundo más importante pero su valor de importancia se reduce a la mitad.

### Predecir 
```{r}
predicciones <- predict(final_model, new_data = indicadores_gen_test)

#preguntar por esta parte
predicciones <- cbind(predicciones, indicadores_gen_test$perc_contr_directa_num)
names(predicciones) <- c(".pred", "perc_contr_directa_num")
```

### Gráfico del test
```{r}
ggplot(predicciones, aes(perc_contr_directa_num, .pred)) +
  geom_point() + 
  geom_abline(intercept = 0, slope = 1) +
  ggtitle("Valor real (contratación directa) Vs predicción (.pred) TEST")

cor.test(predicciones$perc_contr_directa_num, predicciones$.pred)
```




## Random Forest por clasificación

### Preprocesamiento de los datos

Lectura de los datos
```{r}
indicadores_clas <- read.csv("indicadores_BD.csv")
```

Eliminar columnas que no son necesarias
```{r}
indicadores_clas$X <- NULL
indicadores_clas$cod_entidad <- NULL #porque hay muy pccos datos de cada municipio
indicadores_clas$nit_entidad <- NULL
indicadores_clas$cod_departamento <- NULL #porque ya se tiene el ombre del departamento
indicadores_clas$municipio_entidad <- NULL #porque hay muy pocos datos de cada municipio
indicadores_clas$depto_mun <- NULL #porque hay muy pocos datos de cada municipio
indicadores_clas$perc_contr_direct_val <- NULL #porque se supone que no tenemos info de contr. 

#para volver a correr el modelo porque con esta columna genera error 
indicadores_clas$nombre_entidad <- NULL

#pasar a factor las nominales a ver si funciona el modelo
#indicadores_gen$nombre_entidad <- as.factor(indicadores_gen$nombre_entidad)
indicadores_clas$orden_entidad <- as.factor(indicadores_clas$orden_entidad)
indicadores_clas$departamento_entidad <- as.factor(indicadores_clas$departamento_entidad)

# eliminar la columna de los deptos porque no es importante 
#indicadores_gen$departamento_entidad <- NULL
# cuando se elimina los departamentos el r cuadrado baja


# ver si el modelo hace un mejor trabajo solo con los que tienen contr directa >0
#indicadores_gen <- indicadores_gen %>% filter(perc_contr_directa_num>0)


#Ccalcular los cuartiles
cartiles <-quantile(indicadores_clas$perc_contr_directa_num)

# crear la columna de los cuartiles
indicadores_clas <- indicadores_clas %>% 
  mutate(quartil = case_when(perc_contr_directa_num <= cartiles[2] ~ "Q1",
                             perc_contr_directa_num <= cartiles[3] ~ "Q2",
                             perc_contr_directa_num <= cartiles[4] ~ "Q3",
                             TRUE ~ "Q4") )

#verfiicar que si esta funcionando
indicadores_clas %>% dplyr::select(perc_contr_directa_num, quartil)

# hacer histograma con la distribucion de los cuartiles
ggplot(indicadores_clas, aes(perc_contr_directa_num)) + 
  geom_histogram(col = "white", fill = "gray", aes(y = ..density..)) + 
  geom_density(col = "blue") + 
  ggtitle("Distribución de perc_contr_directa_num ") + 
  geom_vline(xintercept = cartiles[2], color = "red") +
  geom_vline(xintercept = cartiles[3], color = "red") +
  geom_vline(xintercept = cartiles[4], color = "red") + 
  geom_vline(xintercept = cartiles[5], color = "red") +
  theme_classic()

#eliminar el porcentaje de contratacion directa
indicadores_clas$perc_contr_directa_num <- NULL
```

División de los datos en train y test
```{r}
set.seed(456)
indicadores_clas_split <- initial_split(indicadores_clas,
                                        strata = quartil)
indicadores_clas_train <- training(indicadores_clas_split)
indicadores_clas_test <- testing(indicadores_clas_split)
```

Definir las tareas de preprocesamiento

- Normalizar las variables numéricas
-  Crear variables dummyes con las variables categóricas
```{r}
indicadores_clas_rec <- recipe(quartil ~ .,
                               data = indicadores_clas) %>% 
  step_normalize(all_numeric()) %>% 
  step_dummy(all_nominal(), -quartil, one_hot = TRUE)

indicadores_clas_rec
```
### Definir el modelo random forest

Se configura un modelo con los parametros que trae la función por defecto.
```{r}
rf_model_clas <-  rand_forest() %>% 
  set_engine("ranger", importance = "impurity") %>% 
  set_mode("classification") 
```

### Definir el workflow
```{r}
rf_workflow_clas <- workflow() %>% 
  add_recipe(indicadores_clas_rec) %>% 
  add_model(rf_model_clas)
```

### Validar el modelo de random forest
```{r}
rf_val_clas <- rf_workflow_clas %>% 
  last_fit(indicadores_clas_split)

rf_val_clas %>% collect_predictions()
```
### Matriz de confusion
```{r}
rf_val_clas %>% collect_predictions() %>% conf_mat(quartil, .pred_class)
```
Algunas métricas a partir de la matriz de confusión y otra información de la predicción.

- accuracy: indica en que proporción se predicen bien las dos clases.
- Sensitivity: indica que proporción de las predicciones positivas son correctas, con respecto al total de clases reales positivas. Specificity: indica que proporción de las predicciones negativas son correctas, con respecto al total de clases reales negativas.
- Positive predictive: indica que proporción de las predicciones positivas son correctas, con respecto al total de predicciones.
- Negative predictive: indica que proporción de las prediccion negativas son correctas, con respecto al total de predicciones.

```{r}
# Métricas generales como el auc y accuracy
rf_val_clas %>% collect_metrics()
```

```{r}
# Sensibilidad
rf_val_clas %>% collect_predictions() %>% sensitivity(quartil, .pred_class)
```

```{r}
# Especificidad
rf_val_clas %>% collect_predictions() %>% specificity(quartil, .pred_class)
```
positiva
```{r}
# Predicción positiva
rf_val_clas %>% collect_predictions() %>% ppv(quartil, .pred_class)
```
negativa
```{r}
# Predicción negativa
rf_val_clas %>% collect_predictions() %>% npv(quartil, .pred_class)
```

A continuación se presenta el número de obervaciones que fueron correctamente clasificadas y el núermo que fue incorrectamente clasificadas
```{r}
#Correcto
rf_val_clas %>% collect_predictions() %>% 
  mutate(correcto = case_when(.pred_class == quartil ~ "Yes",
                              TRUE ~ "No")) %>% 
  filter(correcto == "Yes") %>%
  count()

# incorrecto
rf_val_clas %>% collect_predictions() %>% 
  mutate(correcto = case_when(.pred_class == quartil ~ "Yes",
                              TRUE ~ "No")) %>% 
  filter(correcto == "No") %>%
  count()
```

### Modelo final del random forest
```{r}
rf_final_model_clas <- fit(rf_workflow_clas, training(indicadores_clas_split))
rf_final_model_clas
```

### variables de mayor importancia
```{r}
ranger_obj_clas <- pull_workflow_fit(rf_final_model_clas)$fit

rf_importacia_clas <- tibble(variable = names(ranger_obj_clas$variable.importance),
                            importancia = ranger_obj_clas$variable.importance)

rf_importacia_clas

importantes_clas <- data.frame(rf_importacia_clas %>% 
                             mutate(variable = forcats::fct_reorder(variable, importancia)) %>% 
                            arrange(desc(importancia)))
```
### Accuracy

```{r}
# Nivel de accuracy
rf_metric <- rf_val_clas %>% collect_metrics() %>% mutate(model = "Random Forest")
rf_metric
```

Gráfico de las variables más importantes
```{r}
rf_importacia_clas %>% 
  mutate(variable = forcats::fct_reorder(variable, importancia)) %>% 
   top_n(20) %>% 
  ggplot(aes(variable, importancia)) + 
  geom_bar(stat = "identity") + 
  ggtitle("Importancia de los predictores") +
  coord_flip()
```

